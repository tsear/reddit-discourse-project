{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw\n",
    "import praw\n",
    "import config\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config.CLIENT_ID,\n",
    "    client_secret=config.CLIENT_SECRET,\n",
    "    user_agent=config.USER_AGENT,\n",
    "    username=config.USERNAME,\n",
    "    password=config.PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"philosophy\")\n",
    "for post in subreddit.hot(limit=10):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_posts(subreddit_name, limit=500):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "\n",
    "    for post in subreddit.top(limit=limit, time_filter=\"month\"):\n",
    "        if not post.stickied and not post.locked:\n",
    "            posts.append({\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"id\": post.id,\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"created_utc\": post.created_utc,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"url\": post.url\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = [\"philosophy\", \"askphilosophy\", \"sociology\", \"stoicism\", \"TrueAskReddit\"]\n",
    "all_posts = pd.DataFrame()\n",
    "\n",
    "for sub in subs:\n",
    "    print(f\"Fetching from r/{sub}...\")\n",
    "    df = fetch_posts(sub, limit=500)\n",
    "    all_posts = pd.concat([all_posts, df], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "all_posts.to_csv(\"reddit_philosophy_data.csv\", index=False)\n",
    "print(\"Saved all posts to reddit_philosophy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit_philosophy_data.csv\")\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)        \n",
    "    text = re.sub(r\"\\[.*?\\]\\(.*?\\)\", \"\", text)  \n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     \n",
    "    text = re.sub(r\"\\s+\", \" \", text)                \n",
    "    return text.strip().lower()\n",
    "\n",
    "df[\"clean_title\"] = df[\"title\"].apply(clean_text)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df[\"full_text\"] = df[\"clean_title\"] + \" \" + df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"subreddit\", \"clean_title\", \"clean_text\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_relevant_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"}\n",
    "        and not token.is_stop\n",
    "        and token.is_alpha\n",
    "    )\n",
    "\n",
    "df[\"content_words\"] = df[\"full_text\"].apply(extract_relevant_words)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50,\n",
    "    min_df=2, \n",
    "    max_df=0.8 \n",
    ")\n",
    "\n",
    "subreddit_docs = df.groupby(\"subreddit\")[\"content_words\"].apply(lambda x: \" \".join(x)).to_dict()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(subreddit_docs.values())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=subreddit_docs.keys(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(tfidf_df.T, cmap=\"magma\", annot=True)\n",
    "plt.title(\"Refined TF-IDF Keyword Strength by Subreddit\")\n",
    "plt.xlabel(\"Subreddit\")\n",
    "plt.ylabel(\"Keyword\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "noise_terms = {\"thank\", \"start\", \"bring\", \"happen\", \"use\", \"thing\", \"stuff\", \"someone\", \"everyone\", \"anyone\"}\n",
    "\n",
    "tfidf_df_cleaned = tfidf_df.drop(columns=[col for col in tfidf_df.columns if col in noise_terms])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(tfidf_df_cleaned.T, cmap=\"magma\", annot=True)\n",
    "plt.title(\"Filtered TF-IDF: Discourse-Significant Terms Only\")\n",
    "plt.xlabel(\"Subreddit\")\n",
    "plt.ylabel(\"Keyword\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tfidf_discourse_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_concepts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc \n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\"}\n",
    "            and not token.is_stop\n",
    "            and token.is_alpha]\n",
    "\n",
    "df[\"concepts\"] = df[\"full_text\"].apply(extract_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "co_occurrence = defaultdict(int)\n",
    "\n",
    "for concepts in df[\"concepts\"]:\n",
    "    unique_terms = list(set(concepts))\n",
    "    for pair in itertools.combinations(sorted(unique_terms), 2):\n",
    "        co_occurrence[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for (term1, term2), weight in co_occurrence.items():\n",
    "    if weight >= 3:\n",
    "        G.add_edge(term1, term2, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(nx.degree(G))\n",
    "print(\"Max degree:\", max(degree_dict.values()))\n",
    "print(\"Min degree:\", min(degree_dict.values()))\n",
    "\n",
    "top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "nodes_to_keep = [node for node, _ in top_nodes]\n",
    "\n",
    "G_filtered = G.subgraph(nodes_to_keep).copy()\n",
    "\n",
    "print(\"Filtered nodes:\", len(G_filtered.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_edge_weight = 35\n",
    "\n",
    "G_strong = nx.Graph()\n",
    "\n",
    "for u, v, data in G_filtered.edges(data=True):\n",
    "    if data[\"weight\"] >= min_edge_weight:\n",
    "        G_strong.add_edge(u, v, **data)\n",
    "\n",
    "print(\"Nodes after filtering:\", len(G_strong.nodes))\n",
    "print(\"Edges after filtering:\", len(G_strong.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "pos = nx.spring_layout(G_strong, k=0.9, iterations=150)\n",
    "\n",
    "nx.draw_networkx_nodes(G_strong, pos, node_size=500, node_color=\"orange\", alpha=0.85)\n",
    "nx.draw_networkx_edges(G_strong, pos, width=1.5, alpha=0.4)\n",
    "nx.draw_networkx_labels(G_strong, pos, font_size=11)\n",
    "\n",
    "plt.title(\"Core Concept Co-Occurrence Network\", fontsize=16)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"concept_network.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"full_text\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"subreddit\", y=\"sentiment\", data=df)\n",
    "plt.title(\"Sentiment Distribution by Subreddit\")\n",
    "plt.xlabel(\"Subreddit\")\n",
    "plt.ylabel(\"Compound Sentiment Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_dist.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"NRC-Emotion-Lexicon.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"nrc_emotion_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"nrc_emotion_lexicon\"):\n",
    "    for file in files:\n",
    "        if \"Wordlevel-v0.92.txt\" in file:\n",
    "            full_path = os.path.join(root, file)\n",
    "            print(\"âœ… FOUND:\", full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_lexicon = {}\n",
    "\n",
    "with open(\"nrc_emotion_lexicon/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            word, emotion, score = parts\n",
    "            if int(score) == 1:\n",
    "                emotion_lexicon.setdefault(word, set()).add(emotion)\n",
    "\n",
    "print(list(emotion_lexicon.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_emotions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return Counter()\n",
    "    words = text.lower().split()\n",
    "    emotions = []\n",
    "    for word in words:\n",
    "        if word in emotion_lexicon:\n",
    "            emotions.extend(emotion_lexicon[word])\n",
    "    return Counter(emotions)\n",
    "\n",
    "df[\"emotion_counts\"] = df[\"full_text\"].apply(get_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = df[\"emotion_counts\"].apply(pd.Series).fillna(0)\n",
    "\n",
    "emotion_df[\"subreddit\"] = df[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "emotion_means = emotion_df.groupby(\"subreddit\").mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(emotion_means.T, cmap=\"coolwarm\", annot=True)\n",
    "plt.title(\"Average Emotion Score per Subreddit\")\n",
    "plt.xlabel(\"Subreddit\")\n",
    "plt.ylabel(\"Emotion\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"indepth_sentiment_dist.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names = {\n",
    "    \"nietzsche\", \"kant\", \"plato\", \"socrates\", \"aristotle\", \"foucault\", \n",
    "    \"camus\", \"locke\", \"hume\", \"spinoza\", \"marx\", \"zizek\", \"aurelius\", \n",
    "    \"descartes\", \"heidegger\", \"sartre\", \"chomsky\", \"rawls\", \"mill\", \"hobbes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def match_known_names(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    words = set(text.lower().split())\n",
    "    return [name for name in known_names if name in words]\n",
    "\n",
    "df[\"figures_cited\"] = df[\"full_text\"].apply(match_known_names)\n",
    "name_counts = Counter([name for sublist in df[\"figures_cited\"] for name in sublist])\n",
    "top_cited = name_counts.most_common(20)\n",
    "\n",
    "for name, count in top_cited:\n",
    "    print(f\"{name.title()}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_emotion_profiles = {\n",
    "    \"aurelius\":   {\"trust\": 0.8, \"joy\": 0.4, \"sadness\": 0.1, \"fear\": 0.1, \"positive\": 0.6, \"negative\": 0.1},\n",
    "    \"kant\":       {\"trust\": 0.7, \"joy\": 0.1, \"fear\": 0.2, \"anticipation\": 0.4, \"positive\": 0.3},\n",
    "    \"plato\":      {\"trust\": 0.5, \"anticipation\": 0.3, \"joy\": 0.2},\n",
    "    \"nietzsche\":  {\"anger\": 0.4, \"fear\": 0.5, \"sadness\": 0.3, \"negative\": 0.6},\n",
    "    \"aristotle\":  {\"trust\": 0.6, \"joy\": 0.3, \"positive\": 0.5},\n",
    "    \"socrates\":   {\"trust\": 0.7, \"joy\": 0.2, \"surprise\": 0.2},\n",
    "    \"camus\":      {\"sadness\": 0.5, \"disgust\": 0.3, \"negative\": 0.6},\n",
    "    \"descartes\":  {\"trust\": 0.6, \"anticipation\": 0.4},\n",
    "    \"marx\":       {\"anger\": 0.4, \"disgust\": 0.3, \"sadness\": 0.3, \"negative\": 0.5},\n",
    "    \"spinoza\":    {\"joy\": 0.4, \"trust\": 0.5, \"positive\": 0.5},\n",
    "    \"hume\":       {\"trust\": 0.6, \"joy\": 0.3, \"positive\": 0.4},\n",
    "    \"sartre\":     {\"sadness\": 0.4, \"fear\": 0.3, \"negative\": 0.5},\n",
    "    \"heidegger\":  {\"fear\": 0.5, \"disgust\": 0.3, \"negative\": 0.4},\n",
    "    \"foucault\":   {\"fear\": 0.4, \"disgust\": 0.3, \"sadness\": 0.2, \"negative\": 0.5},\n",
    "    \"hobbes\":     {\"fear\": 0.5, \"anger\": 0.4, \"negative\": 0.5},\n",
    "    \"rawls\":      {\"trust\": 0.6, \"anticipation\": 0.3, \"positive\": 0.4},\n",
    "    \"locke\":      {\"trust\": 0.6, \"joy\": 0.2, \"positive\": 0.3},\n",
    "    \"chomsky\":    {\"anger\": 0.3, \"trust\": 0.5, \"fear\": 0.2},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = df[\"emotion_counts\"].apply(pd.Series)\n",
    "df = pd.concat([df, emotion_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_cols = [\"positive\", \"negative\", \"anger\", \"fear\", \"joy\", \"sadness\", \"trust\", \"anticipation\", \"disgust\", \"surprise\"]\n",
    "\n",
    "df_emotions = df[emotion_cols].copy()\n",
    "df_emotions[\"figures_cited\"] = df[\"figures_cited\"]\n",
    "df_emotions[\"subreddit\"] = df[\"subreddit\"]\n",
    "\n",
    "df_figure_mention = df_emotions.explode(\"figures_cited\").dropna(subset=[\"figures_cited\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "def compare_to_figure_profile(row):\n",
    "    figure = row.name[1]\n",
    "    if figure in figure_emotion_profiles:\n",
    "        figure_profile = figure_emotion_profiles[figure]\n",
    "        \n",
    "        shared_emotions = set(row.index) & set(figure_profile.keys())\n",
    "        if shared_emotions:\n",
    "            subreddit_vector = [row[emotion] for emotion in shared_emotions]\n",
    "            figure_vector = [figure_profile[emotion] for emotion in shared_emotions]\n",
    "            \n",
    "            if any(pd.isna(v) for v in subreddit_vector):\n",
    "                return None\n",
    "            \n",
    "            return euclidean(subreddit_vector, figure_vector)\n",
    "    \n",
    "    return None\n",
    "\n",
    "sub_cite_emotions = df_figure_mention.groupby([\"subreddit\", \"figures_cited\"])[emotion_cols].mean()\n",
    "\n",
    "sub_cite_emotions[\"emotion_distance\"] = sub_cite_emotions.apply(compare_to_figure_profile, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plot_df = sub_cite_emotions.reset_index()\n",
    "\n",
    "plot_df = plot_df.dropna(subset=[\"emotion_distance\"])\n",
    "\n",
    "plot_df = plot_df.sort_values(\"emotion_distance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=plot_df, x=\"figures_cited\", y=\"emotion_distance\", hue=\"subreddit\")\n",
    "\n",
    "plt.title(\"Emotional Distance Between Subreddits and Cited Figures\")\n",
    "plt.xlabel(\"Cited Figure\")\n",
    "plt.ylabel(\"Euclidean Distance in Emotion Profile\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Cited-figures-subreddit.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "distance_df = plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_df[\"subreddit\"].unique())\n",
    "print(distance_df[\"subreddit\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_cols = [\"positive\", \"negative\", \"anger\", \"fear\", \"joy\", \"sadness\", \"trust\", \"anticipation\", \"disgust\", \"surprise\"]\n",
    "\n",
    "# 1. Average emotion intensity per subreddit\n",
    "emotion_means = df.groupby(\"subreddit\")[emotion_cols].mean()\n",
    "\n",
    "# 2. TF-IDF uniqueness: how many high-weight keywords are used\n",
    "tfidf_uniqueness = tfidf_df.apply(lambda row: (row > 0.1).sum(), axis=1)\n",
    "\n",
    "# 3. Citation frequency per subreddit\n",
    "citation_counts = df[\"figures_cited\"].explode().groupby(df[\"subreddit\"]).count()\n",
    "\n",
    "# 4. Mean emotional distance to cited figures\n",
    "mean_emotion_distance = plot_df.groupby(\"subreddit\")[\"emotion_distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_summary = pd.DataFrame({\n",
    "    \"tfidf_uniqueness\": tfidf_uniqueness,\n",
    "    \"citation_count\": citation_counts,\n",
    "    \"mean_emotion_distance\": mean_emotion_distance\n",
    "})\n",
    "\n",
    "subreddit_summary = subreddit_summary.join(emotion_means)\n",
    "\n",
    "subreddit_summary = subreddit_summary.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subreddit_summary.columns)\n",
    "print(subreddit_summary.loc[\"philosophy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_summary = pd.DataFrame(\n",
    "    scaler.fit_transform(subreddit_summary),\n",
    "    columns=subreddit_summary.columns,\n",
    "    index=subreddit_summary.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_radar(data, subreddit, save=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    values = data.loc[subreddit].values\n",
    "    labels = data.columns.tolist()\n",
    "\n",
    "    # Repeat first value to close the loop\n",
    "    values = np.concatenate((values, [values[0]]))\n",
    "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, values, linewidth=2)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=10)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(f\"Discourse Profile: {subreddit}\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        filename = f\"radar_{subreddit}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar(normalized_summary, \"askphilosophy\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in normalized_summary.index:\n",
    "    plot_radar(normalized_summary, sub, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = {}\n",
    "\n",
    "for sub in tfidf_df.index:\n",
    "    top_terms = tfidf_df.loc[sub]\n",
    "    top_terms = top_terms[top_terms > 0]  # Only meaningful terms\n",
    "    top_keywords[sub] = top_terms.sort_values(ascending=False).head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(data, subreddit, top_keywords_dict=top_keywords):\n",
    "    labels = data.columns.tolist()\n",
    "    values = data.loc[subreddit].values.tolist()\n",
    "    values += values[:1]  # loop around\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    ax.plot(angles, values, linewidth=2)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title(f\"Discourse Profile: {subreddit}\", y=1.1)\n",
    "\n",
    "    # âœ… Show keywords at the bottom, inside the figure bounds\n",
    "    keywords = \", \".join(top_keywords_dict.get(subreddit, []))\n",
    "    fig.text(0.5, 0.02, f\"Top terms: {keywords}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    # ðŸ§¼ REMOVE tight_layout â€” it clips text\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"radar_{subreddit}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in normalized_summary.index:\n",
    "    plot_radar(normalized_summary, sub, top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Emotional intensity = std deviation of emotion scores\n",
    "emotion_cols = [\"positive\", \"negative\", \"anger\", \"fear\", \"joy\", \"sadness\", \"trust\", \"anticipation\", \"disgust\", \"surprise\"]\n",
    "subreddit_summary[\"emotion_intensity\"] = df[emotion_cols].groupby(df[\"subreddit\"]).std().mean(axis=1)\n",
    "\n",
    "# 2. Emotional temperature (positive - negative)\n",
    "subreddit_summary[\"emotional_temp\"] = df.groupby(\"subreddit\")[\"positive\"].mean() - df.groupby(\"subreddit\")[\"negative\"].mean()\n",
    "\n",
    "# 3. Normalize relevant columns for scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_cols = [\"tfidf_uniqueness\", \"citation_count\", \"mean_emotion_distance\", \"emotion_intensity\", \"emotional_temp\"]\n",
    "normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(subreddit_summary[scaled_cols]),\n",
    "    columns=[f\"{col}_scaled\" for col in scaled_cols],\n",
    "    index=subreddit_summary.index\n",
    ")\n",
    "\n",
    "# 4. Merge normalized back in\n",
    "subreddit_summary = pd.concat([subreddit_summary, normalized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace with your real values)\n",
    "compass_df = pd.DataFrame({\n",
    "    \"subreddit\": [\"stoicism\", \"askphilosophy\", \"philosophy\", \"TrueAskReddit\", \"sociology\"],\n",
    "    \"x\": [0.1, 0.35, 0.1, 1.0, 0.3],\n",
    "    \"y\": [1.0, 0.48, 0.0, 0.8, 0.44],\n",
    "    \"size\": [300, 500, 100, 600, 400],\n",
    "    \"color\": [0.7, 0.8, 0.2, 1.0, 0.6]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Quadrant background colors\n",
    "colors = {\n",
    "    \"top_left\": \"#c7d5e0\",\n",
    "    \"top_right\": \"#faebd7\",\n",
    "    \"bottom_left\": \"#dcdcdc\",\n",
    "    \"bottom_right\": \"#f2f2f2\"\n",
    "}\n",
    "\n",
    "# Draw quadrant rectangles\n",
    "ax.axhspan(0.5, 1, xmin=0, xmax=0.5, facecolor=colors[\"top_left\"], alpha=0.6, zorder=0)\n",
    "ax.axhspan(0.5, 1, xmin=0.5, xmax=1, facecolor=colors[\"top_right\"], alpha=0.6, zorder=0)\n",
    "ax.axhspan(0, 0.5, xmin=0, xmax=0.5, facecolor=colors[\"bottom_left\"], alpha=0.6, zorder=0)\n",
    "ax.axhspan(0, 0.5, xmin=0.5, xmax=1, facecolor=colors[\"bottom_right\"], alpha=0.6, zorder=0)\n",
    "\n",
    "# Plot subreddit bubbles\n",
    "scatter = ax.scatter(\n",
    "    compass_df[\"x\"], compass_df[\"y\"],\n",
    "    s=compass_df[\"size\"],\n",
    "    c=compass_df[\"color\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.9,\n",
    "    edgecolors='black'\n",
    ")\n",
    "\n",
    "# Subreddit name labels\n",
    "for i, row in compass_df.iterrows():\n",
    "    ax.text(\n",
    "        row[\"x\"], row[\"y\"] + 0.05,\n",
    "        row[\"subreddit\"],\n",
    "        ha=\"center\", va=\"bottom\",\n",
    "        fontsize=10, color=\"black\", weight=\"bold\"\n",
    "    )\n",
    "\n",
    "# Add quadrant labels with semi-transparent background\n",
    "def label_quadrant(x, y, text):\n",
    "    ax.text(\n",
    "        x, y, text,\n",
    "        ha=\"center\", va=\"center\",\n",
    "        fontsize=10, color=\"black\", weight=\"bold\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"none\", boxstyle=\"round,pad=0.3\")\n",
    "    )\n",
    "\n",
    "label_quadrant(0.25, 0.75, \"Detached / Reflective\")\n",
    "label_quadrant(0.75, 0.75, \"Expressive / Experiential\")\n",
    "label_quadrant(0.25, 0.25, \"Detached / Cold\")\n",
    "label_quadrant(0.75, 0.25, \"Ideological / Abstract\")\n",
    "\n",
    "# Axis dividers\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Axis labels & title\n",
    "ax.set_title(\"ðŸ§­ The Discourse Compass: Mapping Reddit Philosophy Subcultures\", fontsize=14, weight=\"bold\", pad=20)\n",
    "ax.set_xlabel(\"Conceptual Anchoring (Closer to Cited Thinkers â†’)\", fontsize=11)\n",
    "ax.set_ylabel(\"Emotional Expressiveness (Muted â†’ Intense)\", fontsize=11)\n",
    "\n",
    "# Add color legend\n",
    "norm = plt.Normalize(0, 1)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Emotional Temperature\", fontsize=10)\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"discourse_compass_final_refined.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
